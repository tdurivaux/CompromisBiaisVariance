{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectif : comprendre le compromis biais-variance\n",
    "\n",
    "<a href=\"https://xkcd.com/1725/\"><img src=\"img/linear_regression xkcd.png\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Concept et modélisation\n",
    "\n",
    "## Motivations\n",
    "\n",
    "En Machine Learning, un compromis existe entre la volonté de reproduire avec précision les régularités des données d'apprentissage et le souhait de généraliser au-delà de l'échantillon d'apprentissage pour que le modèle traite efficacement d'autres données (les données de test).\n",
    "\n",
    "Ce problème central en apprentissage supervisé s'applique aussi bien aux problèmes de classification qu'aux problèmes de régression. Nommé *compromis biais-variance*, il traduit en fait le lien entre deux sources d'erreurs : le biais et la variance. \n",
    "\n",
    "- L'**erreur de biais** représente l'erreur entre la prédiction attendue de notre modèle et la valeur exacte que nous tentons de prédire. \n",
    "\n",
    "- L'**erreur de variance** représente la variabilité de la prédiction pour un point donné entre plusieurs réalisations.\n",
    " \n",
    "Plus les données d'apprentissage sont reproduites par le modèle avec précision plus le biais est faible, mais l'on risque de reproduire le bruit lié au jeu de données : on fait alors du sur-apprentissage. À l'inverse, généraliser de façon excessive diminue la variance du modèle mais augmente l'erreur de représentation des données d'apprentissage : on parle alors de sous-apprentissage.\n",
    "\n",
    "Comprendre le *compromis biais-variance* peut aider à mieux interpréter les résultats de nos modèles de prédiction, améliorer les divers paramètres des algorithmes d'apprentissage et éviter les problèmes de sur- et sous-apprentissage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition mathématique : décomposition de l'erreur quadratique\n",
    "\n",
    "Soit un échantillon d'apprentissage constitué d'un ensemble de points $X=(x_1,..., x_n)$ et les valeurs réelles associées $Y=(y_1,...,y_n)$ que l'on cherche à prédire. On peut supposer que nos données suivent un modèle statistique de la forme :\n",
    "$$ Y = f(X) + \\epsilon $$\n",
    "où $\\epsilon$ représente un terme d'erreur suivant une loi normale : $\\epsilon \\sim \\mathcal{N}(0,\\sigma_\\epsilon)$\n",
    "\n",
    "Un modèle $\\hat{f}$ de $f$ peut alors être estimé grâce à un algorithme d'apprentissage supervisé. Dans notre cas, l'erreur quadratique de prédiction pour un échantillon $x$ est :\n",
    "$$Err(x) = E\\left[(y - \\hat{f}(x))^2\\right]$$\n",
    "\n",
    "En développant l'expression (remplaçant $y$ par $f(x)+\\epsilon$), on peut alors décomposer cette erreur, que l'on souhaite minimiser, en plusieurs composantes :\n",
    "$$ Err(x) = \\underbrace{\\left( E[ \\hat{f}(x)] - f(x) \\right)}_{\\text{biais}}~^2 + \\underbrace{E\\left[\\left(\\hat{f}(x) - E[\\hat{f}(x)]\\right)^2\\right]}_{\\text{variance}} + \\underbrace{\\sigma_\\epsilon^2}_{\\text{erreur irréductible}} $$ \n",
    "Soit, en résumé :\n",
    "$$ Err(x) = Biais(\\hat{f})^2 + Variance(\\hat{f}) + Err_{\\text{irréductible}} $$\n",
    "\n",
    "Le premier terme, le *biais* au carré, mesure l'ampleur de la différence entre la moyenne des prédictions obtenues avec l'ensemble des données et la valeur souhaitée de la fonction $f$. Le second terme, la *variance*, mesure à quel point les solutions obtenues à partir de jeux de données distincts varient autour de leur moyenne, et par conséquent dans quelle mesure la fonction de prédiction $\\hat{f}$ est sensible aux données d'apprentissage. Le dernier terme, l'erreur irréductible, correspond au bruit intrinsèque des données d'apprentissage : celui-ci ne peut être réduit contrairement au biais et à la variance.\n",
    "\n",
    "Avec un modèle parfait, on pourrait annuler le biais et la variance simultanément. La perfection des modèles étant plutôt utopique dans la réalité, cette décomposition met en évidence le *compromis biais-variance* (lié à la complexité des modèles) que l'on doit réaliser. Un modèle de $\\hat{f}$ complexe présentera un faible biais mais la flexibilité qu'il entraîne implique souvent une forte variance. A l'inverse un modèle rigide possèdera souvent un fort biais et une faible variance. Les contributions respectives du biais et de la variance sur l'erreur totale peuvent être représentées suivant une courbe en \"baignoire\":\n",
    "\n",
    "<img src=\"img/biasvariance.png\"/>\n",
    "\n",
    "Le modèle « optimal » de prédiction est alors le modèle qui présente le meilleur *compromis biais-variance*.\n",
    "Les différents paramètres des modèles utilisés en apprentissage supervisé vont permettre d'agir sur l'une ou l'autre des sources d'erreur. C'est alors à l'auteur de faire varier ces paramètres pour trouver le meilleur compromis entre fidélité aux données et généralisation.\n",
    "\n",
    "Pour agir sur l'erreur d'apprentissage, on peut :\n",
    "- jouer sur les paramètres internes du modèle ;\n",
    "- agir sur les données d'apprentissage.\n",
    "\n",
    "Commençons par quelques `import`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn import neighbors, svm, datasets\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Un premier exemple : régression polynômiale\n",
    "\n",
    "Pour une première approche du **compromis biais-variance**, nous allons chercher à approximer la fonction $f : x\\mapsto\\sin(2\\pi x)+\\sin(5\\pi (x+0.5))$ par des polynômes de différents degrés.\n",
    "\n",
    "La fonction `genSample` genère un échantillon de $n$ points bruités autour de la fonction $f$. La fonction `plotFunction` affiche la fonction de référence (elle admet un paramètre de couleur optionnel), et `plotPoly` affiche le polynôme passé en argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(X):\n",
    "    return np.sin(2*np.pi*X)+np.sin(5*np.pi*(X+0.5))\n",
    "\n",
    "def genSample(n):\n",
    "    \"\"\" Génère un échantillon aléatoire de taille n \"\"\"\n",
    "    X = np.linspace(0, 1, n)\n",
    "    Y = f(X) + np.random.randn(n)/3\n",
    "    return X, Y\n",
    "\n",
    "def plotFunction(color=\"blue\"):\n",
    "    \"\"\" Trace la fonction f \"\"\"\n",
    "    N = 100\n",
    "    X = np.linspace(0, 1, N+1)\n",
    "    Y = f(X)\n",
    "    plt.plot(X, Y, c=color, label=\"fonction f\")\n",
    "    return\n",
    "\n",
    "def plotPoly(p, color=\"orange\"):\n",
    "    \"\"\" Trace le polynôme p \"\"\"\n",
    "    N = 100\n",
    "    X = np.linspace(0, 1, N+1)\n",
    "    Y = np.polyval(p, X)\n",
    "    plt.plot(X, Y, c=color, label=\"polynôme\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "**Consigne :** Dans un premier temps, complétez le code ci-dessous afin de déterminer et afficher le polynôme de degré 5 approximant au mieux l'échantillon généré (*cf* aide de **numpy**).</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = genSample(20)\n",
    "plt.scatter(X, Y, c=\"red\", label=\"données apprentissage\")\n",
    "plotFunction()\n",
    "\n",
    "### à compléter\n",
    "\n",
    "_=plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "**Consigne :** Ensuite, complétez les fonctions suivantes :\n",
    "<ul>\n",
    "    <li/> `genPolys` : génère $N$ polynômes de degré $d$ approximant $N$ échantillons de taille $n$ ;\n",
    "    <li/> `plotMeanPolys` : trace la moyenne des polynômes donnés en arguments.\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def genPolys(N, d, n):\n",
    "    ### à compléter\n",
    "    return ### à compléter\n",
    "\n",
    "def plotMeanPolys(polys, color=\"red\"):\n",
    "    ### à compléter\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "**Consigne :** Maintenant que les fonctions nécessaires sont disponibles, nous allons mettre en évidence le *compromis biais-variance* par le biais (!) du degré du polynôme de régression. Pour cela, nous prendrons $n = 20$ la taille des échantillons, et $N = 100$ le nombre d'échantillons. Pour chaque degré $d$ entre 5 et 12 :\n",
    "<ul>\n",
    "    <li/>générez $N$ échantillons de taille $n$ ;\n",
    "    <li/>approximez-les par $N$ polynômes de degré $d$ ;\n",
    "    <li/>tracez en quelques-uns (par exemple 10) sur une figure ;\n",
    "    <li/>tracez la fonction $f$ et le polynôme moyen obtenu sur une autre figure.\n",
    "</ul>\n",
    "<br/>\n",
    "Enfin, tracez le logarithme (pour obtenir un graphe lisible) du biais et de la variance en fonction du degré des polynômes de régression.\n",
    "<br/>\n",
    "(si cela semble vous prendre plus d'une quinzaine de minutes, jetez un œil à la correction ;-) )\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dmin = 5\n",
    "dmax = 12\n",
    "biais2 = []\n",
    "variance = []\n",
    "N = 100 # nombre d'échantillons\n",
    "n = 20 # nombre de points par échantillon\n",
    "X = np.linspace(0, 1, n)\n",
    "\n",
    "fig, axes = plt.subplots(dmax-dmin+1, 2, figsize=(20,40))\n",
    "plt.axes(axes[0,0])\n",
    "    \n",
    "for d in range(dmin, dmax+1):\n",
    "    \n",
    "    ### à compléter\n",
    "    \n",
    "    # Affichage de quelques approximations\n",
    "    ax = axes[int(d-dmin), 0]\n",
    "    plt.axes(ax)\n",
    "    ax.set_title(\"Degré {} - quelques approximations\".format(d))\n",
    "    for i in range(0, N, N//10):\n",
    "        plotPoly(polys[i])\n",
    "    \n",
    "    # Approximation moyenne et référence\n",
    "    ax = axes[int(d-dmin), 1]\n",
    "    plt.axes(ax)\n",
    "    ax.set_title(\"Approximation (rouge) & fonction (bleue)\")\n",
    "    plotMeanPolys(polys)\n",
    "    plotFunction()\n",
    "    ax.legend()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20,6))\n",
    "axes[0].semilogy(range(dmin, dmax+1), variance, c=\"green\")\n",
    "axes[0].set_title(\"Variance (log) en fonction du degré du polynôme\")\n",
    "axes[1].semilogy(range(dmin, dmax+1), biais2, c=\"green\")\n",
    "_ = axes[1].set_title(\"Biais (log) en fonction du degré du polynôme\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "**Questions :**\n",
    "<ul>\n",
    "    <li/>Comment évoluent le biais et la variance ?\n",
    "    <li/>Comment ces grandeurs sont-elles traduites sur les figures tracées auparavant ?\n",
    "    <li/>Quel degré choisireriez-vous pour ce problème ?\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De façon générale, il pourrait être tentant de chercher à minimer le biais au prix de la variance, de sorte à avoir un modèle « en moyenne juste ». Cependant, en présence d'un unique échantillon et notamment s'il n'y a pas d'apprentissage ultérieur, il est préférable d'équilibrer biais et variance : *in fine*, ce qui compte est la performance sur les données que l'on a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Un second exemple de régression : *k-nearest neighbours*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons utiliser ici l'algorithme des *k-nearest neighbours*, toujours en régression, pour approximer la même fonction $f$ que dans l'exemple précédent.\n",
    "Cet algorithme est très simple : la prédiction de la valeur de $f$ en $x$ est la moyenne des valeurs prises en les $k$ voisins les plus proches.\n",
    "$$ \\hat{f}(x) = \\frac{1}{k}\\sum_{i=1}^k f\\left(N_i(x)\\right) $$\n",
    "avec $N_i(x)$ le $i$-ème voisin le plus proche de $x$ dans l'ensemble d'apprentissage.\n",
    "\n",
    "Il se trouve qu'il existe une formule explicite simple de l'erreur pour *k-nearest neighbours* :\n",
    "$$ Err(x) = \\left(f(x)-\\frac{1}{k}\\sum_{i=1}^{k}f\\left(N_i(x)\\right)\\right)^2+\\frac{\\sigma_\\epsilon^2}{k}+\\sigma_\\epsilon^2 $$\n",
    "où $\\sigma$ est l'écart-type du bruit auquel est soumis l'ensemble d'apprentissage : $y_i = f(x_i) + \\epsilon$, $\\epsilon \\sim \\mathcal{N}(0,\\sigma_\\epsilon)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "**Question :** Comment évoluent biais et variance en fonction de $k$ d'après cette formule ?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "**Consigne :** Complétez le code ci-dessous pour tester *k-nearest neighbours* avec divers $k$ (ici de 1 à 9, de 2 en 2) et calculer la variance sur $N$ échantillons.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Données d'apprentissage\n",
    "X = np.random.rand(30)\n",
    "Y = f(X)+np.random.rand(30)/3\n",
    "\n",
    "# Xf utilisé pour f et pour la prédiction\n",
    "Xf = np.linspace(0,1,100)\n",
    "\n",
    "# Image de Xf par f\n",
    "Yf = f(Xf)\n",
    "\n",
    "X = X.reshape(-1,1)\n",
    "Y = Y.reshape(-1,1)\n",
    "Xf = Xf.reshape(-1,1)\n",
    "\n",
    "# Nombre d'échantillons\n",
    "N = 20\n",
    "\n",
    "kmin, kmax = 1, 10 # de 2 en 2...\n",
    "Ypredict = np.empty((N, (kmax-kmin+1)//2, len(Xf)))\n",
    "\n",
    "for i in range(N):\n",
    "    for k in range(kmin, kmax, 2):\n",
    "        ### créez ici le prédicteur avec neighbors de scikit (déjà importé)\n",
    "        \n",
    "        Ypredict[i, k//2, :] = None ### à compléter : calcul de la prédiction\n",
    "        \n",
    "        if i==1: # on affiche un échantillon pour chaque k\n",
    "            plt.figure()\n",
    "            plt.plot(Xf, Yf, c='blue', label=\"fonction f\")\n",
    "            plt.plot(Xf, Ypredict[i,int(k/2),:], c='orange', label=\"prédiction\")\n",
    "            plt.scatter(X, Y, c='red', label=\"données apprentissage\")\n",
    "            plt.title(\"k={}\".format(k))\n",
    "            _=plt.legend()\n",
    "\n",
    "for k in range(kmin, kmax, 2):\n",
    "    Yvar = None # à compléter : calcul de la variance\n",
    "    \n",
    "    print(\"Variance moyenne pour k = {} sur N = {} essais : {:0.2f}\".format(k,N,np.mean(Yvar)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "**Questions :** La variance et le biais évoluent-ils bien comme prévu ? Comment se traduit le biais sur les figures précédentes ?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exemple de classification avec *Support Vector Machines*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rappels\n",
    "\n",
    "Le principe des algorithmes de SVM repose sur la recherche d'un hyperplan séparateur pour classifier un jeu de données en différentes catégories. Le meilleur hyperplan correspond à celui qui maximise la distance entre l'hyperplan et un ensemble de points, dits vecteurs supports, qui sont les plus proches de l'hyperplan. C'est également l'hyperplan qui minimise l'erreur de généralisation. \n",
    "\n",
    "Cependant, dans le cas de données non séparables, il est nécessaire d'introduire une marge autorisant une mauvaise classification d'un certain nombre de points. Un paramètre $C$ est alors introduit pour contrôler cette erreur de classification. Il constitue le paramètre clef dans la gestion du **compromis biais-variance** dans le cadre des algorithmes d'aprentissage supervisé SVM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Influence du paramètre C\n",
    "\n",
    "On crée une fonction `generateData` qui génère aléatoirement un échantillon de données appartenant à deux catégories distinctes. Le but de cette partie est de construire un hyperplan séparateur pour ce jeu de donné et d'étudier l'influence du paramètre $C$ sur le compromis biais-variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateData(n=500):\n",
    "    X1, y1 = datasets.make_gaussian_quantiles(cov=2.0, n_samples=n, n_features=2, n_classes=1)\n",
    "    X1[:,0] = 2. + X1[:,0]#2\n",
    "    X1[:,1] = 6. + X1[:,1]/2.5#6\n",
    "    X2, y2 = datasets.make_gaussian_quantiles(cov=1.5, n_samples=n, n_features=2, n_classes=1)\n",
    "    X2[:,0] = 4 + X2[:,0]#7.8\n",
    "    X2[:,1] = 4. + X2[:,1]#4\n",
    "    X3, y3 = datasets.make_gaussian_quantiles(cov=1.5, n_samples=n, n_features=2, n_classes=1)\n",
    "    X3[:,0] = 5 + X3[:,0]#5\n",
    "    X3[:,1] = 9. + X3[:,1]#9\n",
    "    X = np.concatenate((X1, X2, X3))\n",
    "    y = np.concatenate((y1, -y2 + 1, y3))\n",
    "    y = 2*y-1\n",
    "    X, y = shuffle(X, y)\n",
    "    return X, y\n",
    "\n",
    "# Affichage\n",
    "X, y = generateData(20)\n",
    "Xyellow = X[y==-1]\n",
    "Xblue = X[y==1]\n",
    "plt.figure()\n",
    "plt.scatter(Xyellow[:,0], Xyellow[:,1], c='y', s=5)\n",
    "_=plt.scatter(Xblue[:,0], Xblue[:,1], c='b', s = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"> **Consigne :** Pour commencer, complétez l'algorithme calculant le meilleur hyperplan séparateur pour différents $C$, i.e. différentes tolérances d'erreur de classification ($C$ gère le compromis entre le nombre d'erreurs de classement et la largeur de la marge). On utilisera un séparateur linéaire.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_param = [0.001, 0.05, 1]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15,5))\n",
    "\n",
    "for i, C in enumerate(C_param):\n",
    "    ### à compléter : création du SVC et apprentissage\n",
    "\n",
    "    w = svc.coef_[0]\n",
    "    w0 = svc.intercept_\n",
    "    M = 1./np.linalg.norm(w)\n",
    "\n",
    "    # On affiche le séparateur, la marge, les vecteurs support\n",
    "    # Points\n",
    "    ax = axes[i]\n",
    "    ax.scatter(Xyellow[:,0], Xyellow[:,1], c='y', s=5)\n",
    "    ax.scatter(Xblue[:,0], Xblue[:,1], c='b', s=5)\n",
    "    # Séparateur avec marge\n",
    "    XX = np.arange(-1., 12., 0.1)\n",
    "    YY = -(w[0]*XX+w0)/w[1]\n",
    "    ax.plot(XX, YY, 'g')\n",
    "    YY = -(w[0]*XX+w0+M)/w[1]\n",
    "    ax.plot(XX, YY, 'g--')\n",
    "    YY = -(w[0]*XX+w0-M)/w[1]\n",
    "    ax.plot(XX, YY, 'g--')\n",
    "    # Vecteurs supports\n",
    "    ax.scatter(svc.support_vectors_[:,0], svc.support_vectors_[:,1], s=15, edgecolors='k', c='')\n",
    "    _=ax.set_title('C={}'.format(C_param[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"> **Consigne :** Maintenant, tracez l'évolution du biais et de la variance (déjà implémentées ci-dessous) avec les paramètres suivants :\n",
    "<ul>\n",
    "    <li>$N$ échantillons d'apprentissage, pour autant de SVC à entraîner</li>\n",
    "    <li>$n\\_train$ points par échantillon d'apprentissage</li>\n",
    "    <li>$n\\_test\\times3$ points dans l'échantillon de test</li>\n",
    "</ul>\n",
    "Le modèle aggrégeant tous les SVC entraînés est un simple vote majoritaire.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_param = np.logspace(-2, 4, 20)\n",
    "N = 50 # nombre de SVC\n",
    "n_train = 15 # taille du tiers de l'échantillon d'apprentissage\n",
    "\n",
    "n_test = 1000 # taille de l'échantillon de test\n",
    "\n",
    "ypred = np.zeros((N, len(C_param), n_test*3)) # prédictions\n",
    "\n",
    "Xtest, ytest = generateData(n_test) # test\n",
    "\n",
    "for n in range(N):\n",
    "    ### Entraînement des SVC (pour chaque C) et prédiction\n",
    "\n",
    "bias = np.zeros(len(C_param))\n",
    "var = np.zeros(len(C_param))\n",
    "\n",
    "# On crée la prédiction majoritaire\n",
    "for i in range(len(C_param)):\n",
    "    mainPrediction = np.sum(ypred[:, i, :], axis=0)\n",
    "    mainPrediction[mainPrediction < N/2] = -1\n",
    "    mainPrediction[mainPrediction != -1] = 1\n",
    "    \n",
    "    # Calcul du biais et de la variance pour la loss function binaire\n",
    "    # Voir [3] (page 732) pour l'origine du calcul\n",
    "    bias[i] = np.sum((mainPrediction != ytest)[:n_test*3])/(n_test*3)\n",
    "    var[i] = np.sum(np.sum(np.repeat(mainPrediction, N).reshape(n_test*3, N).transpose() != ypred[:, i, :]))/n_test/N\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel(\"Paramètre C\")\n",
    "plt.loglog(C_param, bias, label=\"Biais\")\n",
    "plt.loglog(C_param, var, label=\"Variance\")\n",
    "_=plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">**Question :** Interprétez les erreurs dans les cas de $C$ extrêmes. Faites le lien avec le biais et la variance. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un $C$ beaucoup trop faible (comme $0.001$ tracé plus haut) mène à un SVC qui classe tous les points dans une seule classe. Cela explique les valeurs de biais « aberrantes » parfois visibles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Une façon de s'affranchir du *compromis biais-variance*\n",
    "\n",
    "La méthode de **Bagging**, et en particulier l'algorithme de **Random Forest**, permet de réduire la variance sans modifier le biais.\n",
    "\n",
    "Pour rappel, le Bagging (*Bootstrap Aggregating*) consiste en la création de nouvelles données d'apprentissage en ré-échantillonnant l'échantillon initial. L'algorithme de Random Forest est l'usage du Bagging pour des arbres de décision complètement développés avec une sélection des *features* aléatoire.\n",
    "\n",
    "Un arbre de décision (complètement développé) a un biais faible mais une variance très forte (du fait du détail atteint à chaque niveau de décision). En en combinant plusieurs (à la façon Random Forest), la variance diminue (la moyenne de nombreux modèles bruités donne un résultat peu bruité) sans que le biais ne change.\n",
    "\n",
    "Le biais restant inchangé, la seule limite à la réduction de variance est le temps de calcul disponible.\n",
    "\n",
    "Nous allons rapidement illustrer cette réduction de la variance sans changer le biais (en d'autres termes : la compensation du sur-apprentissage des arbres de décision) avec l'exemple simple du notebook sur Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1, y1 = datasets.make_gaussian_quantiles(cov=2.,\n",
    "                                          n_samples=300,\n",
    "                                          n_features=2,\n",
    "                                          n_classes=2,\n",
    "                                          random_state=1)\n",
    "X2, y2 = datasets.make_gaussian_quantiles(mean=(3, 3),\n",
    "                                          cov=1.5,\n",
    "                                          n_samples=700,\n",
    "                                          n_features=2,\n",
    "                                          n_classes=2,\n",
    "                                          random_state=1)\n",
    "X = np.concatenate((X1, X2))\n",
    "y = np.concatenate((y1, - y2 + 1))\n",
    "y = 2*y-1\n",
    "\n",
    "X, y = shuffle(X, y)\n",
    "\n",
    "Xtest,X = np.split(X, [400])\n",
    "ytest,y = np.split(y, [400])\n",
    "\n",
    "Xblue = X[y==-1]\n",
    "Xred = X[y==1]\n",
    "plt.scatter(Xblue[:,0], Xblue[:,1], c='b')\n",
    "_=plt.scatter(Xred[:,0], Xred[:,1], c='r')\n",
    "\n",
    "def plot_decision_boundary(f, X, y, title=\"\"):\n",
    "    plot_step = 0.02\n",
    "    x0_min, x0_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x1_min, x1_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx0, xx1 = np.meshgrid(np.arange(x0_min, x0_max, plot_step), np.arange(x1_min, x1_max, plot_step))\n",
    "    yypred = f.predict(np.c_[xx0.ravel(), xx1.ravel()])\n",
    "    yypred = yypred.reshape(xx0.shape)\n",
    "    plt.contourf(xx0, xx1, yypred, cmap=plt.cm.Paired)\n",
    "    y_pred = f.predict(X)\n",
    "    Xblue_good = X[np.equal(y, -1) * np.equal(y, y_pred)]\n",
    "    Xblue_bad = X[np.equal(y, -1) * np.not_equal(y, y_pred)]\n",
    "    Xred_good = X[np.equal(y, 1) * np.equal(y, y_pred)]\n",
    "    Xred_bad = X[np.equal(y, 1) * np.not_equal(y, y_pred)]\n",
    "    plt.scatter(Xblue_good[:, 0], Xblue_good[:,1], c='b')\n",
    "    plt.scatter(Xblue_bad[:, 0], Xblue_bad[:,1], c='c', marker='v')\n",
    "    plt.scatter(Xred_good[:, 0], Xred_good[:,1], c='r')\n",
    "    plt.scatter(Xred_bad[:, 0], Xred_bad[:,1], c='m', marker='v')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"> **Consignes :** Entraînez et affichez quelques (4 par exemple) forêts avec un seul arbre. Observez la variabilité du modèle (avec *plot_decision_boundary*) entre chacune de ces tentatives.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### votre code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"> **Consigne :** De façon similaire à la partie sur SVM, tracez l'évolution du biais et de la variance (déjà implémentées ci-dessous).\n",
    "\n",
    "Le modèle aggrégeant les forêts aléatoires est un simple vote majoritaire.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxTrees = 50 # nombre maximal d'arbre par forêt\n",
    "n_rf = 20 # nombre de forêts à entraîner\n",
    "N = len(Xtest) # taille de l'échantillon de test\n",
    "\n",
    "ypred = np.zeros((n_rf, maxTrees, N)) # prédictions\n",
    "\n",
    "bias = np.zeros(maxTrees)\n",
    "var = np.zeros(maxTrees)\n",
    "\n",
    "for nbTrees in range(maxTrees):\n",
    "    ### Création d'une forêt aléatoire\n",
    "    \n",
    "    for n in range(n_rf):\n",
    "        ### Apprentissage de la forêt aléatoire et enregistrement des prédictions pour (n, nbTrees)\n",
    "\n",
    "for i in range(maxTrees):\n",
    "    mainPrediction = np.sum(ypred[:, i, :], axis=0)\n",
    "    mainPrediction[mainPrediction < n_rf/2] = -1\n",
    "    mainPrediction[mainPrediction != -1] = 1\n",
    "    \n",
    "    # Calcul du biais et de la variance pour la loss function binaire\n",
    "    # Voir [3] (page 732) pour l'origine du calcul\n",
    "    bias[i] = np.sum((mainPrediction != ytest)[:n_rf])/N\n",
    "    var[i] = np.sum(np.sum(np.repeat(mainPrediction, n_rf).reshape(N, n_rf).transpose() != ypred[:, i, :]))/N/n_rf\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel(\"Nombre d'arbres\")\n",
    "plt.plot(range(1, maxTrees+1), bias, label=\"Biais\")\n",
    "plt.plot(range(1, maxTrees+1), var, label=\"Variance\")\n",
    "_=plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliographie\n",
    "\n",
    "[1] __Understanding the Bias-Variance Tradeoff__ - Scott Fortmann Roe - June 2012 - \n",
    "http://scott.fortmann-roe.com/docs/BiasVariance.html\n",
    "\n",
    "[2] __The Elements of Statistical Learning__ - _Data Mining, Inference and Prediction ; 2nd Edition_ - Trevor Hastie, Robert Tibshirani, Jerome Friedman - https://web.stanford.edu/~hastie/Papers/ESLII.pdf\n",
    "\n",
    "[3] __Bias-variance analysis of support vector machines for the development of SVM-based ensemble methods__ - _Journal of Machine Learning Research_ (725-775) - Valentini, G., & Dietterich, T. G. - 5 July 2004 - http://www.jmlr.org/papers/volume5/valentini04a/valentini04a.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auteurs de ce notebook\n",
    "\n",
    "Thibaud Durivaux & Aymeline Martin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
